# ===============================
# 1. IMPORTS
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error

from statsmodels.tsa.statespace.sarimax import SARIMAX

import torch
import torch.nn as nn
import torch.optim as optim

# ===============================
# 2. SYNTHETIC MULTIVARIATE DATA
# ===============================
np.random.seed(42)

n = 1200
t = np.arange(n)

trend = 0.01 * t
seasonal_1 = np.sin(2 * np.pi * t / 30)
seasonal_2 = 0.5 * np.sin(2 * np.pi * t / 90)

hetero_noise = np.random.normal(0, 0.2 + 0.002 * t)

target = trend + seasonal_1 + hetero_noise
aux_feature = seasonal_2 + np.random.normal(0, 0.1, n)

data = pd.DataFrame({
    "target": target,
    "aux_feature": aux_feature
})

# ===============================
# 3. VISUALIZATION
# ===============================
data.plot(subplots=True, figsize=(12,6), title="Synthetic Multivariate Time Series")
plt.show()

# ===============================
# 4. TRAIN–TEST SPLIT
# ===============================
train_size = int(len(data) * 0.8)
train = data.iloc[:train_size]
test = data.iloc[train_size:]

# ===============================
# 5. BASELINE MODEL 1 — SARIMA
# ===============================
sarima = SARIMAX(
    train["target"],
    order=(2,1,2),
    seasonal_order=(1,1,1,30)
)
sarima_fit = sarima.fit(disp=False)
sarima_pred = sarima_fit.forecast(len(test))

# ===============================
# 6. EVALUATION METRICS
# ===============================
def evaluate(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return mae, rmse, mape

sarima_metrics = evaluate(test["target"], sarima_pred)

# ===============================
# 7. SCALING FOR DEEP LEARNING
# ===============================
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# ===============================
# 8. SEQUENCE CREATION (MULTIVARIATE)
# ===============================
def create_sequences(data, seq_len=30):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len, :])
        y.append(data[i+seq_len, 0])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data)

# ===============================
# 9. DL TRAIN–TEST SPLIT
# ===============================
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1,1)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1,1)

# ===============================
# 10. BASELINE MODEL 2 — SIMPLE LSTM
# ===============================
class SimpleLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(2, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1])

lstm_model = SimpleLSTM()
criterion = nn.MSELoss()
optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)

for epoch in range(15):
    optimizer.zero_grad()
    loss = criterion(lstm_model(X_train), y_train)
    loss.backward()
    optimizer.step()

# ===============================
# 11. ATTENTION-BASED LSTM (MAIN MODEL)
# ===============================
class AttentionLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(2, 64, batch_first=True)
        self.attn = nn.Linear(64, 1)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        weights = torch.softmax(self.attn(lstm_out), dim=1)
        context = torch.sum(weights * lstm_out, dim=1)
        return self.fc(context), weights

attn_model = AttentionLSTM()
optimizer = optim.Adam(attn_model.parameters(), lr=0.001)

for epoch in range(20):
    optimizer.zero_grad()
    preds, _ = attn_model(X_train)
    loss = criterion(preds, y_train)
    loss.backward()
    optimizer.step()

# ===============================
# 12. PREDICTIONS
# ===============================
with torch.no_grad():
    lstm_preds = lstm_model(X_test).numpy()
    attn_preds, attn_weights = attn_model(X_test)

# Inverse scaling
lstm_preds = scaler.inverse_transform(
    np.c_[lstm_preds, np.zeros(len(lstm_preds))]
)[:,0]

attn_preds = scaler.inverse_transform(
    np.c_[attn_preds.numpy(), np.zeros(len(attn_preds))]
)[:,0]

# ===============================
# 13. FINAL METRICS COMPARISON
# ===============================
results = pd.DataFrame({
    "Model": ["SARIMA", "LSTM", "Attention LSTM"],
    "MAE": [
        sarima_metrics[0],
        evaluate(test["target"], lstm_preds)[0],
        evaluate(test["target"], attn_preds)[0]
    ],
    "RMSE": [
        sarima_metrics[1],
        evaluate(test["target"], lstm_preds)[1],
        evaluate(test["target"], attn_preds)[1]
    ],
    "MAPE": [
        sarima_metrics[2],
        evaluate(test["target"], lstm_preds)[2],
        evaluate(test["target"], attn_preds)[2]
    ]
})

print(results)

# ===============================
# 14. FORECAST VISUALIZATION
# ===============================
plt.figure(figsize=(12,4))
plt.plot(test["target"].values, label="Actual")
plt.plot(attn_preds, label="Attention LSTM Forecast")
plt.legend()
plt.title("Forecast vs Actual")
plt.show()

# ===============================
# 15. ATTENTION WEIGHTS ANALYSIS
# ===============================
avg_attention = attn_weights.mean(dim=0).squeeze().numpy()

plt.figure(figsize=(10,4))
plt.plot(avg_attention)
plt.title("Average Attention Weights Across Time Steps")
plt.xlabel("Time Step")
plt.ylabel("Attention Weight")
plt.show()
