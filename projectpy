# ==============================
# 1. INSTALL REQUIRED LIBRARIES
# ==============================
!pip install shap statsmodels scikit-learn tensorflow --quiet

# ==============================
# 2. IMPORT LIBRARIES
# ==============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

import statsmodels.api as sm
import shap

# ==============================
# 3. GENERATE COMPLEX TIME SERIES DATA
# ==============================
np.random.seed(42)
t = np.arange(0, 1200)

trend = t * 0.005
season1 = 15 * np.sin(2 * np.pi * t / 30)
season2 = 8 * np.sin(2 * np.pi * t / 120)
noise = np.random.normal(0, 2, len(t))

data = trend + season1 + season2 + noise
df = pd.DataFrame({"value": data})

plt.figure(figsize=(10,4))
plt.plot(df['value'])
plt.title("Synthetic Complex Time Series")
plt.show()

# ==============================
# 4. NORMALIZATION
# ==============================
scaler = MinMaxScaler()
df['scaled'] = scaler.fit_transform(df[['value']])

# ==============================
# 5. CREATE SEQUENCES (WINDOWING)
# ==============================
def create_sequences(data, window=40):
    X, y = [], []
    for i in range(len(data) - window):
        X.append(data[i:i+window])
        y.append(data[i+window])
    return np.array(X), np.array(y)

X, y = create_sequences(df['scaled'].values)
X = X.reshape((X.shape[0], X.shape[1], 1))

# ==============================
# 6. TRAIN-TEST SPLIT
# ==============================
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ==============================
# 7. LSTM DEEP LEARNING MODEL
# ==============================
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(X_train.shape[1],1)),
    Dropout(0.3),
    LSTM(64),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()

# ==============================
# 8. TRAIN MODEL (HYPERPARAMETER TUNING)
# ==============================
history = model.fit(
    X_train, y_train,
    epochs=25,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# ==============================
# 9. PREDICTIONS
# ==============================
preds = model.predict(X_test)
preds_inv = scaler.inverse_transform(preds)
y_test_inv = scaler.inverse_transform(y_test.reshape(-1,1))

# ==============================
# 10. EVALUATION METRICS
# ==============================
rmse_lstm = np.sqrt(mean_squared_error(y_test_inv, preds_inv))
print("LSTM RMSE:", rmse_lstm)

naive = y_test_inv[:-1]
mase_lstm = np.mean(np.abs(y_test_inv[1:] - preds_inv[1:])) / np.mean(np.abs(y_test_inv[1:] - naive))
print("LSTM MASE:", mase_lstm)

# ==============================
# 11. BASELINE MODEL (SARIMA)
# ==============================
train_series = df['value'][:split]
test_series = df['value'][split:]

sarima = sm.tsa.SARIMAX(train_series, order=(1,1,1), seasonal_order=(1,1,1,30))
sarima_model = sarima.fit(disp=False)

sarima_preds = sarima_model.forecast(len(test_series))
rmse_sarima = np.sqrt(mean_squared_error(test_series, sarima_preds))
print("SARIMA RMSE:", rmse_sarima)

# ==============================
# 12. EXPLAINABILITY (SHAP)
# ==============================
explainer = shap.DeepExplainer(model, X_train[:100])
shap_values = explainer.shap_values(X_test[:50])
shap.summary_plot(shap_values[0], X_test[:50])

# ==============================
# 13. FINAL MODEL COMPARISON
# ==============================
results = pd.DataFrame({
    "Model": ["LSTM", "SARIMA"],
    "RMSE": [rmse_lstm, rmse_sarima],
    "MASE": [mase_lstm, "N/A"]
})
print("\nFinal Comparison:")
print(results)
