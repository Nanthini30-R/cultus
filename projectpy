# ==============================
# 1. INSTALL LIBRARIES
# ==============================
!pip install statsmodels scikit-learn tensorflow --quiet

# ==============================
# 2. IMPORT LIBRARIES
# ==============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm

# ==============================
# 3. GENERATE MULTIVARIATE TIME SERIES
# ==============================
np.random.seed(42)
t = np.arange(0, 1500)

trend = t * 0.003
season1 = 10 * np.sin(2*np.pi*t/24)
season2 = 5 * np.sin(2*np.pi*t/168)
noise = np.random.normal(0, 1.5, len(t))

target = trend + season1 + season2 + noise

lag_feature = np.roll(target, 1)
season_feature = np.sin(2*np.pi*t/168)
noise_feature = np.random.normal(0,1,len(t))

df = pd.DataFrame({
    "target": target,
    "lag_feature": lag_feature,
    "season_feature": season_feature,
    "noise_feature": noise_feature
}).dropna()

# ==============================
# 4. NORMALIZATION
# ==============================
scaler = StandardScaler()
scaled = scaler.fit_transform(df)

# ==============================
# 5. WINDOWING (48 â†’ 24)
# ==============================
def create_sequences(data, input_len=48, output_len=24):
    X, y = [], []
    for i in range(len(data) - input_len - output_len):
        X.append(data[i:i+input_len])
        y.append(data[i+input_len:i+input_len+output_len, 0])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled)

# ==============================
# 6. TRAIN TEST SPLIT
# ==============================
split = int(0.8*len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# ==============================
# 7. SEQ2SEQ LSTM MODEL
# ==============================
def build_model(units=64, dropout=0.2):
    model = tf.keras.Sequential([
        tf.keras.layers.LSTM(units, return_sequences=True, input_shape=(48,4)),
        tf.keras.layers.Dropout(dropout),
        tf.keras.layers.LSTM(units),
        tf.keras.layers.Dense(24)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

# ==============================
# 8. GRID SEARCH
# ==============================
configs = [(64,0.2,32),(128,0.3,64)]
best_loss = np.inf

for units,drop,batch in configs:
    model = build_model(units,drop)
    h = model.fit(X_train,y_train,epochs=10,batch_size=batch,
                  validation_split=0.2,verbose=0)
    val_loss = min(h.history['val_loss'])
    if val_loss < best_loss:
        best_loss = val_loss
        best_model = model

model = best_model

# ==============================
# 9. PREDICTIONS
# ==============================
preds = model.predict(X_test)

# inverse transform only target
y_test_inv = scaler.inverse_transform(
    np.concatenate([y_test.reshape(-1,1),
                    np.zeros((y_test.size,3))],axis=1)
)[:,0].reshape(y_test.shape)

preds_inv = scaler.inverse_transform(
    np.concatenate([preds.reshape(-1,1),
                    np.zeros((preds.size,3))],axis=1)
)[:,0].reshape(preds.shape)

# ==============================
# 10. METRICS
# ==============================
rmse_lstm = np.sqrt(mean_squared_error(y_test_inv.flatten(),
                                       preds_inv.flatten()))

naive = y_test_inv[:, :-1]
mase_lstm = np.mean(np.abs(y_test_inv[:,1:] - preds_inv[:,1:])) / \
            np.mean(np.abs(y_test_inv[:,1:] - naive))

print("LSTM RMSE:", rmse_lstm)
print("LSTM MASE:", mase_lstm)

# ==============================
# 11. SARIMA BASELINE (MATCH HORIZON)
# ==============================
train_series = df['target'][:split+48]
sarima = sm.tsa.SARIMAX(train_series,
                       order=(1,1,1),
                       seasonal_order=(1,1,1,24))
sarima_model = sarima.fit(disp=False)

sarima_preds = sarima_model.forecast(steps=len(y_test.flatten()))
rmse_sarima = np.sqrt(mean_squared_error(y_test_inv.flatten(),
                                        sarima_preds))

print("SARIMA RMSE:", rmse_sarima)

# ==============================
# 12. INTEGRATED GRADIENTS
# ==============================
@tf.function
def integrated_gradients(inputs, baseline, steps=50):
    alphas = tf.linspace(0.0,1.0,steps)
    grads = []
    for alpha in alphas:
        x = baseline + alpha*(inputs-baseline)
        with tf.GradientTape() as tape:
            tape.watch(x)
            pred = model(x)
        grads.append(tape.gradient(pred,x))
    avg_grads = tf.reduce_mean(tf.stack(grads),axis=0)
    return (inputs-baseline)*avg_grads

baseline = tf.zeros_like(X_test[:1])
ig = integrated_gradients(X_test[:1],baseline)

importance = tf.reduce_mean(tf.abs(ig),axis=[0,1])
print("Feature importance:", importance.numpy())

# ==============================
# 13. FINAL COMPARISON
# ==============================
print("\nFINAL RESULTS")
print("LSTM RMSE:", rmse_lstm)
print("SARIMA RMSE:", rmse_sarima)
print("LSTM MASE:", mase_lstm)
